# -*- coding: utf-8 -*-
"""htn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15lLLlDjpjABeXtnod_vmBYPuHpIx52ZQ
"""

#!pip install tensorflow

#!pip install tensorflow tensorflow-gpu opencv-python matplotlib

import tensorflow as tf
import os 
import yaml

# Avoid OOM errors by setting GPU Memory Consumption Growth
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

tf.config.list_physical_devices('GPU')

import cv2
import imghdr

data_dir = 'htn'

image_exts = ['jpeg','jpg', 'bmp', 'png']

for image_class in os.listdir(data_dir):
    for image in os.listdir(os.path.join(data_dir, image_class)):
        image_path = os.path.join(data_dir, image_class, image)
        try:
            img = cv2.imread(image_path)
            tip = imghdr.what(image_path)
            if tip not in image_exts:
                print('Image not in ext list {}'.format(image_path))
                os.remove(image_path)
        except Exception as e:
            print('Issue with image {}'.format(image_path))
            # os.remove(image_path)

import numpy as np
from matplotlib import pyplot as plt

from keras.models import Sequential

from tensorflow.python.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D
from tensorflow.python.keras.models import Model

from keras.preprocessing import image

# Create an image dataset from the directory
try:
    data = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        image_size=(128, 128),  # Specify image size first
        batch_size=32,          # Then specify batch size
        validation_split=0.2,   # Adjust validation split as needed
        subset='training',
        seed=42                # Specify a seed for reproducibility
    )

    validation_data = tf.keras.utils.image_dataset_from_directory(
        data_dir,
        image_size=(128, 128),  # Specify image size first
        batch_size=32,
        validation_split=0.2,
        subset='validation',
        seed=42                # Specify the same seed for validation data
    )

    class_names = data.class_names

    # Continue with model building and training as described in the previous response

except Exception as e:
    print(f"An error occurred: {str(e)}")

model = tf.keras.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(len(class_names), activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(data, validation_data=validation_data, epochs=20) #update: just 10 was enough

history = model.history

# Evaluate the model on the validation dataset
loss, accuracy = model.evaluate(validation_data)
print(f'Validation accuracy: {accuracy * 100:.2f}%')

import matplotlib.pyplot as plt
import networkx as nx
import json

fig = plt.figure()
plt.plot(history.history['loss'], color='teal', label='loss')
plt.plot(history.history['val_loss'], color='orange', label='val_loss')
fig.suptitle('Loss', fontsize=20)
plt.legend(loc="upper left")
plt.show()

plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

for images, labels in data:
    predictions = model.predict(images)
    predicted_labels = tf.argmax(predictions, axis=1)

    # Iterate through the batch and display each image along with its predicted label
    for i in range(len(images)):
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(f'Predicted: {class_names[predicted_labels[i]]}, True: {class_names[labels[i]]}')
        plt.show()

model.save('CNN.h5')

with open("class_names.yaml", "w") as file: 
    yaml.dump(class_names, file)

